{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating some data\n",
    "In order to do this lab you're gonna need some data. Holding to the tradition of data science, let's make it up as we go. Using PyTorch's __[built in distribution sampling functions](https://pytorch.org/docs/stable/distributions.html)__ create a dataset sampled from a Bernoulli, and another dataset sampled from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "First, draw a sample from a normal distribution using the parameters below. Store your data in the variable named `X_norm`, and print out the sample mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import normal\n",
    "\n",
    "trueMu = 1.491# mean\n",
    "trueSig = 0.1876# standard deviation\n",
    "nNormal = 1000\n",
    "#### your code here ####\n",
    "\n",
    "\n",
    "normal_dist = normal.Normal(loc = trueMu, scale = trueSig)\n",
    "X_norm = normal_dist.sample([nNormal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How close are the sample estimates of the mean and standard deviation to the true values? How can you improve the sample estimation of these values? What is the theorem that backs up your answer? Please describe how you might do that below and then implement your solution.** Continue to store your data in the variable `X_norm`.\n",
    "\n",
    "To improve the sample estimates of the mean and standard deviation, we could increase the sample size. This is the **Law of Large Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1858)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of sample\n",
    "X_norm.mean()\n",
    "\n",
    "# std. of sample\n",
    "X_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary data generation\n",
    "Follow the same steps as above, but instead of a normal distribution we want binary data. What distribution can you use to sample binary data? Make sure that the sample estimate probability of seeing a value of `1` is close to `trueP`. **Please plot the mean of your sample as you increase n. Say from 10 to 10,000** and confirm that the mean does converge to `trueP`. Remember to store your largest sample in the variable `X_bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12627fc18>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trueP = 0.23\n",
    "\n",
    "bernoulli_dist = torch.distributions.bernoulli.Bernoulli(probs=trueP)\n",
    "\n",
    "N, mu = [], []\n",
    "for n in range(10,10000):\n",
    "    N.append(n)\n",
    "    mu.append(bernoulli_dist.sample([n]).mean())\n",
    "    \n",
    "plt.plot(N, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Maximum Likelihood Estimation\n",
    "You'll be learning more about maximum likelihood estimation (MLE) next week, but you'll have a chance to gain some intuition, and see how one can implement MLE here.\n",
    "\n",
    "In the previous section you learned how more data can help you estimate the parameters of distributions that observations are drawn from. In the example of data drawn from a normal distribution it's intuitive how we can estimate the mean and standard deviation which parameterize the distribution which generated the data. Put another way, given the data we've seen and an idea for the function which generated this data (ie a normal distribution) we want find the parameters that likely describe how this data was generated. \n",
    "\n",
    "Let's examine some of the moving parts. We have some data, $X \\in R^{n}$, and we know that each observation $x^{i}$ was generated with probability defined by $f(x^{i};\\theta)$ where $\\theta$ are the parameters that help us define this function. I know this is a little handwavy but just bear with me here. You'll get a better theoretical understanding in class. Let's look at the example of the normal distribution as a data generating function. Here $f(x^{i};\\theta) = f(x^{i};\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x^{i}-\\mu)^2}{2\\sigma^2})$ which is the probability density function of the __[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)__. \n",
    "\n",
    "So that's the probability of observing one $x^i$, but what about all $n$ of those dudes? Well since we're dealing with indpendently and identically distributed data (or independent trials of data drawn from the same distribution) we just multiply that function over and over to get the likelihood of observing our data. Recall that $f(X;\\mu, \\sigma) = f(x^1, x^2,...,x^n;\\mu, \\sigma)$ is a joint distribution over all the variables in $X$. Because of independence this joint can be rewritten as the product of the likelihood of every $x^i$. That is $f(X;\\mu, \\sigma) = \\prod_{i = 1}^{n}f(x^{i}|\\mu, \\sigma) = \\frac{1}{2\\pi\\sigma^2}^{\\frac{n}{2}}exp(-\\frac{\\sum_{i=1}^{n}(x^{i}-\\mu)^2}{2\\sigma^2})$. The last step skips over some algebra when multiplying the probability density function of the normal $n$ times. \n",
    "\n",
    "Ok, so that was alot. But essentially we're just deriving the probability of seeing all our data. The next and final step is to take the log of the final equation above. We do this because of numerical stability. When you're dealing with small numbers, like fractions, repeatedly muliplying them can lead to underflow. $$L(X;\\mu, \\sigma) = \\frac{-n}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\pi\\sigma^2}\\sum_{i=1}^{n}(x^i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During MLE our goal is to find the parameters $\\theta$ which maximize the function $L(X;\\theta)$. In the cell below fix $\\sigma$ at the true value of `0.1876` and calculate the likelihood function for the data observed with varied $\\mu$ between 1 and 2 with a step size of 0.05. Which value of $\\mu$ maximizes the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.498999999999945"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump\n",
    "\n",
    "def MLE(X, truSig):\n",
    "    # evaluate the likelihood of the observed vector given a range of mu's\n",
    "    likelihoodList, muList = [], []\n",
    "    for mu in frange(1,2,0.001):\n",
    "        likelihood_est = []\n",
    "        \n",
    "        # estimate the likelihood of each observed number GIVEN mu\n",
    "        for x in X:\n",
    "            likelihood_est.append(0.5*np.log(2*math.pi*(truSig**2)) - 1/2*math.pi*(truSig*truSig) - ((x - mu)**2))\n",
    "    \n",
    "        likelihoodList.append(sum(likelihood_est))\n",
    "        muList.append(mu)\n",
    "        \n",
    "    index = likelihoodList.index(max(likelihoodList))\n",
    "    return muList[index]\n",
    "\n",
    "\n",
    "truSig = 0.1876\n",
    "MLE(X = X_norm, truSig = 0.1876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Normal\n",
    "So from above we can see that at the true values of $\\sigma$ and $\\mu$ the log likelihood is maximized. How can we learn what these parameters should be in order to maximize the MLE? With a little PyTorch and gradient descent we can do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors, Variables and Computational graphs\n",
    "Insert Wizard of Oz \"Oh my\" joke here. Ok, moving on. If we're going to learn the parameters which maximize the MLE, we need to know which parameters to maximize, and how to run the MLE function as well as how to perform back propagation. Let's look at each of these pieces separately. \n",
    "\n",
    "### Tensors\n",
    "You can kind of think of Tensors and PyTorch's specialized NumPy matrices. How are they like NumPy matrices? They... well... they can hold values in a matrix (single or multidimensional) format. And how are the specialized? Unlike NumPy matrices Tensors can make use of GPU resources to significantly speed up matrix operations, which make up a bulk of what you need to do in deep learning.\n",
    "\n",
    "### Variables (or Tensors with a gradient)\n",
    "It used to be that PyTorch had a separate object called a Variable which were just like Tensors, except they're meant to hold values which will change as we learn their optimal or \"true\" value. In the most current version of PyTroch these two objects have been merged into the Tensor object. When you have a Tensor which needs to store data, but also needs to be updated as we learn its optimal or \"true\" value you create a tensor but require that the gradient be stored. You'll see this in the code below.\n",
    "\n",
    "Tensors hold onto a few important attributes which you can access through `Tensor.grad.data` and `Tensor.data`. One holds onto the actual values of the object, while the other holds onto the gradient. As we learn more about machine learning in this class the reason why these two pieces of information are important will become clear. One important thing to know is that every time a variable is back propogated through the gradient **accumulates** and is not replaced so you have to manually zero out the gradient with `Tensorgrad.data.zero_()`.\n",
    "\n",
    "\n",
    "### Computational graphs\n",
    "Computational graphs are structures which store chains of multiplications, additions, and other math functions (sin, exponential, log, Hadamard product, etc) in such a way that calculating the gradient via the chain rule is simple. Again, you will learn more about why it's important for us to do this in the future, but for now all you need to know is that PyTorch creates computational graphs which allow you to easily calculate the derivative using the chain rule. \n",
    "\n",
    "A computational graph is created only once you run code which starts applying math functions, multiplications, and additions to Tensors. This means it's a dynamic computation graph. And once you calculate the derivative using a backward pass it's destroyed, meaning that all the information needed to calculate the derivatives is gone. There's more to this, but if you want to learn more about it I would suggest this __[video](https://www.youtube.com/watch?v=nbJ-2G2GXL0)__ and this __[site](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)__.\n",
    "\n",
    "## An example \n",
    "Let's see some of this in action. I've implemented code to learn how to maximize the MLE for data drawn from a normal distribution by learning the best $\\mu$ and $\\sigma$ parameters. But let's face it, it's gonna take me a while to figure out what I'm doing. So there are 4 versions of code below. All but the last one has a bug in it that prevents it from doing what we want. **Identify the bug in each version, and what needs to be fixed**. Of course you can peak ahead to see what changed, but looking at each version by itself before moving on to the next will be helpful for future debugging sessions.\n",
    "\n",
    "Tip: The third example might be kind of hard to spot because the code runs, but it never learns anything. Why might this be? What's going on with the gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 1\n",
    "#### What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.0895], requires_grad=True)\n",
      "Initial Sigma: tensor([0.8170], requires_grad=True)\n",
      "log likelihood: [-2207.212], learned mu = [0.08950484], learned sigma [0.8169567]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-59e7db8f3325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mMLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearnedMu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmyIter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mMLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Now we run over the data multiple times, and each time we calculate how far away we are from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# the maximized MLE and update our parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=True)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "#Instantiate the MLE for a normal distribution here using torch functions.\n",
    "MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "for myIter in range(1000):\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "        \n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 2\n",
    "#### What went wrong?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.5995], requires_grad=True)\n",
      "Initial Sigma: tensor([0.0468], requires_grad=True)\n",
      "log likelihood: [-185520.06], learned mu = [0.5994688], learned sigma [0.04684776]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-90ff5d757475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mMLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearnedMu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmyIter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mMLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Now we run over the data multiple times, and each time we calculate how far away we are from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# the maximized MLE and update our parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "#Instantiate the MLE for a normal distribution here using torch functions.\n",
    "MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "for myIter in range(1000):\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 3\n",
    "#### What went wrong?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.1832], requires_grad=True)\n",
      "Initial Sigma: tensor([0.6837], requires_grad=True)\n",
      "log likelihood: [-2395.7485], learned mu = [0.18319565], learned sigma [0.6837497]\n",
      "log likelihood: [-410.60504], learned mu = [1.2467116], learned sigma [0.4988375]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n",
      "log likelihood: [254.87772], learned mu = [1.4874781], learned sigma [0.18753003]\n"
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.00001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    # computes the derivative on the MLE (arguments required for non-scalars)\n",
    "    MLE.backward()\n",
    "    \n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate\n",
    "    \n",
    "    # Clear out the gradient information\n",
    "    learnedMu.grad.data.zero_()\n",
    "    learnedSigma.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.2935], requires_grad=True)\n",
      "Initial Sigma: tensor([0.1176], requires_grad=True)\n",
      "log likelihood: [-51579.707], learned mu = [0.29346043], learned sigma [0.11761582]\n",
      "log likelihood: [-1442.1178], learned mu = [0.48137885], learned sigma [1.0186422]\n",
      "log likelihood: [-1355.0919], learned mu = [0.5742584], learned sigma [1.0113331]\n",
      "log likelihood: [-1275.4534], learned mu = [0.6608977], learned sigma [0.9902608]\n",
      "log likelihood: [-1196.769], learned mu = [0.7434682], learned sigma [0.95800716]\n",
      "log likelihood: [-1114.7399], learned mu = [0.82350075], learned sigma [0.9157205]\n",
      "log likelihood: [-1025.5133], learned mu = [0.9022334], learned sigma [0.8636316]\n",
      "log likelihood: [-924.6259], learned mu = [0.98082495], learned sigma [0.8011927]\n",
      "log likelihood: [-805.7887], learned mu = [1.0605413], learned sigma [0.7269671]\n",
      "log likelihood: [-658.69226], learned mu = [1.1430044], learned sigma [0.6382165]\n"
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate\n",
    "    # Clear out the gradient information\n",
    "    learnedMu.grad.data.zero_()\n",
    "    learnedSigma.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Bernoulli\n",
    "Finally, using the binary data we generated using a Bernoulli distribution let's learn the true value for $p$. Similarly to the above formulation you'll need to create a parameter to learn, as well as define the MLE for our binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueP = 0.23\n",
    "X_benoulli = torch.distributions.bernoulli.Bernoulli(probs=trueP).sample([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.9576], requires_grad=True)\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n",
      "log likelihood: [0.], learned p = [0.9575607]\n"
     ]
    }
   ],
   "source": [
    "# We'll wrap our randomly generated data in a Variable call\n",
    "# Setting requires_grad=False means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables \n",
    "x = torch.autograd.Variable(X_benoulli, requires_grad = False)\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the MLE.\n",
    "learnedP = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedP))\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.0001\n",
    "n = X_benoulli.shape[0]\n",
    "\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    MLE = ((learnedP)**torch.sum(x))*((1-learnedP)**(x_length - torch.sum(x)))\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned p = {}\".format(MLE.data.numpy(),\n",
    "                                                          learnedP.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedP.data = learnedP.data + learnedP.grad.data*learningRate\n",
    "    # Clear out the gradient information\n",
    "    learnedP.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)\n\n",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
