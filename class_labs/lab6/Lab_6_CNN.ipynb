{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![I_made_this_in_facebook_messenger_I_kid_you_not](https://raw.githubusercontent.com/crowegian/memes/master/ImATerribleTA.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Convolutional Neural Networks\n",
    "Lab 6 will follow a lot of the same steps that lab 5 did, but instead of using a fully connected neural network (FCNN) you'll be using a convolutional neural network (CNN). CNNs work well when the inputs have spatial patterns that can be learned. Images fall into this category as neighboring pixels are often correlated. CNNs follow a basic form of INPUT -> CONV_LAYER(CONV -> ACTIVATION_FUNCTION -> POOL)*d -> FCNN where CONV_LAYER can have d layers and the FCNN at the end can be as deep or shallow as you want. \n",
    "\n",
    "For the rest of this explanation I'm about to drop some knowledge straight out of [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1). All images are from this site, unless otherwise noted.\n",
    "\n",
    "## Convolutions\n",
    "The main workhorse of the CNN is the kernel. The kernel is a learnable matrix, which is used to pick out (indicated by a strong activation) visual features. Below is an example of a 3x3 kernel (dark blue) convolving over the width and height an image to produce an output (grey).\n",
    "![2dConvSingleKernel](img/2dConvKernel.gif)\n",
    "\n",
    "The kernel is essentially a weighted sum over the inputs. A more concrete example is below.\n",
    "![2dConvWithNums](img/2dConvWithNums.gif)\n",
    "\n",
    "\n",
    "## Striding\n",
    "Sometimes when we're performing convolutions we want the output to be smaller than the input. This can be achieved by increasing the size of our kernels (not the best way to achieve arbitrary shrinkage) or making the kernel skip over some pixels when convolving across an image also known as striding.\n",
    "\n",
    "![2dConvWithStriding](img/2dConvWithStriding.gif)\n",
    "\n",
    "The image above has a stride of two, which reduces the output shape by a factor of 2. A stride of 3 would reduce the output shape by a factor of 3, and so on.\n",
    "\n",
    "Another way to reduce size is with a pooling operation\n",
    "\n",
    "## Pooling\n",
    "\n",
    "Pooling is a technique to introduce invariance to small translations as well as reduce the output size. There are many forms of pooling using averaging, L2-norms, but max pooling is the most common. An example of max pooling with a stride of 2 over the output of each convolution of a 2x2 kernel is below.\n",
    "\n",
    "![maxpool](img/maxpool.jpeg)\n",
    "[source](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "\n",
    "## Padding\n",
    "Now you know two ways of reducing the size of each layer using kernels, and striding. But if you keep reducing size eventually you'll have nothing to work with and you can't make arbitrarily deep models. Padding is a technique where you concatenate pixels to the edges of an image, so that the output after a convlutional layer is that same size as the input. This can also be done during a pooling step, but that's not super common.\n",
    "![2dConvWithPadding](img/2dConvWithPadding.gif)\n",
    "\n",
    "## Output shapes\n",
    "Kernel shape, padding, and stride all affect the output shape after a convolution, or pooling layer. Here are some formulas to help you figure out the shape of the output for a square input. Let $O$ be the output shape, $W$ be the input shape, $K$ be the kernel size, $P$ be the amount of padding, and $S$ the stride. \n",
    "\n",
    "### After a convolution layer\n",
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "\n",
    "### After a pooling layer\n",
    "$$O = \\frac{W - K}{S} + 1$$\n",
    "\n",
    "## Output Channels\n",
    "Why limit yourself to only one kernel of a specific size? PyTorch allows you to create multiple multiple output channels for each kernel of the same size. In this case, your output is no longer a 2d matrix, but a matrix with width, height, and depth depending on the number of channels used. Below is an example of a single kernel convolving over a single 2 dimensional space, with 5 output channels. \n",
    "\n",
    "![outputChannelDepth](img/outputChannelDepth.jpeg)\n",
    "[source](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "## Input Channels\n",
    "![colorChannels](img/colorChannels.jpeg)\n",
    "In this assignemnt we're still dealing with black and white images which have only one channel (grey). Basically this part isn't relevant to this lab, but hey, why not stick around since you're already here. Dealing with color images means you have three input channels (RGB). So how do you perform 2d convolutions over this kind of input? Let's assume we're only dealing with a single output channel, in this case there will be one unique (trainable) kernel for each input channel. This collection of kernels is known as a filter. Each kernel then convolves over each input channel.\n",
    "![multiInputChannel](img/multiInputChannel.gif)\n",
    "\n",
    "The filter then produces multiple outputs (one for each kernel), which are summed together to produce a single output for this filter. There's also a bias term, which is the same value added to each kernel.\n",
    "![filterSumming](img/filterSumming.gif)\n",
    "\n",
    "Hey, look at you. You made it all the way here. And even after I told you that it wasn't necessary for the lab. Take a moment to feel good about yourself. Remember to take some time everyday to reflect on what you've done well, even if it's making your bed, brushing your teeth, or taking time to read a book. Give yourself some credit, you deserve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the module `torchsummary` with `pip install torchsummary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import inspect\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data\n",
    "Since you already learned how to pull data from PyTorch I'll do that for you. However, do notice how I'm using the transform argument. I'm performing this on the testing dataset to make things a little harder, but you could also do this on the training dataset to augment your data with randomly rotated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                            train=False, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download = True)\n",
    "\n",
    "# in case you wanted to load in the data without any rotation\n",
    "# train_dataset = dsets.MNIST(root='./data', \n",
    "#                             train=True, \n",
    "#                             transform=transforms.Compose([\n",
    "#                                 transforms.RandomRotation(degrees = 10),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 ]),\n",
    "#                             download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                            train=False, \n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.RandomRotation(degrees = 25),\n",
    "                                transforms.ToTensor(),\n",
    "                                ]),\n",
    "                            download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.train_data.size())\n",
    "print(train_dataset.train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.test_data.size())\n",
    "print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 1876\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(test_loader))\n",
    "\n",
    "plotData = x[0][0:25]\n",
    "labelData = x[1][0:25]\n",
    "\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i + 1, xmargin = 2.0)\n",
    "    plt.xlabel(str(labelData[i].numpy()))\n",
    "    plt.imshow(plotData[i][0].numpy())\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vanilla CNN\n",
    "Fill in the code below to build a model following these instructions.\n",
    "\n",
    "1. INPUT -> \n",
    "2. CONV(outputChannels = 16, kernelSize = 5, stride=1, padding = 0) -> ReLU -> MAXPOOL(kernelSize=2) ->\n",
    "3. CONV(outputChannels = 32, kernelSize = 5, stride=1, padding = 0) -> ReLU MAXPOOL(kernelSize=2) ->\n",
    "4. FCNN(32 * 4 * 4, 10)\n",
    "\n",
    "In the forward pass you'll have to reshape your outputs to fit into a 2 dimensional FCNN. Check out the view() function from PyTorch.\n",
    "The code below was snagged from www.deeplearningwizard.com, but you should be able to figure out how to code this up without peaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelStatic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModelStatic, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        #### your code here ###\n",
    "        self.cnn1 = \n",
    "        \n",
    "        # ReLU 1\n",
    "        #### your code here ###\n",
    "        self.relu1 = \n",
    "\n",
    "        # Max pool 1\n",
    "        #### your code here ###\n",
    "        self.maxpool1 =\n",
    "\n",
    "        # Convolution 2\n",
    "        #### your code here ###\n",
    "        self.cnn2 = \n",
    "        \n",
    "        # RelU 2\n",
    "        #### your code here ###\n",
    "        self.relu2 =\n",
    "\n",
    "        # Max pool 2\n",
    "        #### your code here ###\n",
    "        self.maxpool2 = \n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        #### your code here ###\n",
    "        self.fc1 = \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Reshape your output layer to fit into the final layer of the FCNN\n",
    "        # Original size: (batchSize, 32, 4, 4)\n",
    "        # out.size(0): batchSize\n",
    "        # New out size: (batchSize, 32*4*4)\n",
    "        #### your code here ###\n",
    "        \n",
    "        #### end code here ###\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Model Summary\n",
    "Using the summary function from torchsummary print out the layers, shapes, and number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dynamic CNN\n",
    "The above code is great for getting an idea of how to build a model. But in production we would want to be able to iterate through multiple different architectural choices like number of layers, filter sizes, pooling kernels, strides, activation functions, etc. In order to be able to do this we need to build our model using arguments which specify the architecture and loops. Otherwise we'd have to explicitly type out every layer of our model, for every change we wanted to implement.\n",
    "\n",
    "Now just in case any of your are getting flashbacks to lab 4, not to worry. I've implemented most of the function for you. You're encouraged to walk through it to understand what's happening. **All you have to do is implement the method `calculateFinalOutputSize` that calculates the size of the final output, which is needed to calculate the size of the final FCNN layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNModelDynamic(nn.Module):\n",
    "    def __init__(self, input_shape = 28, n_classes = 10,\n",
    "                 in_channels_list = [1, 16], out_channels_list = [16, 32],\n",
    "                 kernel_size_list = [5, 5], stride_list = [1, 1],\n",
    "                 padding_list = [0,0], pool_kernel_list = [2, 2],\n",
    "                 pool_stride_list = [2, 2],\n",
    "                 pooling_list = [nn.MaxPool2d, nn.MaxPool2d], activations_list = [nn.ReLU(), nn.ReLU()]):\n",
    "        super(CNNModelDynamic, self).__init__()\n",
    "        localArgs = locals().items()\n",
    "        argLens = set()\n",
    "        ignoredArgs = ['self', \"__class__\", \"input_shape\", \"n_classes\"]\n",
    "        for argName, arg in localArgs:\n",
    "            if argName not in ignoredArgs:\n",
    "                argLens.add(len(arg))\n",
    "        assert len(argLens) == 1, (\"mismatch in lengths of arguments.\"\n",
    "                                   \"All params for each layer must be specified\")\n",
    "        finalOutputSize = self.calculateFinalOutputSize(input_shape,kernel_size_list, stride_list,\n",
    "                                         padding_list, pool_kernel_list, pool_stride_list)\n",
    "        modules = list()\n",
    "        for layerIdx in range(0, argLens.pop()):\n",
    "            modules.append(nn.Conv2d(in_channels = in_channels_list[layerIdx],\n",
    "                                 out_channels = out_channels_list[layerIdx],\n",
    "                                 kernel_size = kernel_size_list[layerIdx],\n",
    "                                 stride = stride_list[layerIdx],\n",
    "                                 padding = padding_list[layerIdx]))\n",
    "            modules.append(activations_list[layerIdx])\n",
    "            modules.append(pooling_list[layerIdx](kernel_size = pool_kernel_list[layerIdx],\n",
    "                                                  stride = pool_stride_list[layerIdx]))\n",
    "        self.convolutions = nn.Sequential(*modules)\n",
    "        self.finalLayer = nn.Linear(finalOutputSize**2*out_channels_list[-1], n_classes)\n",
    "        \n",
    "        \n",
    "    def calculateFinalOutputSize(self, input_shape, kernel_size_list, stride_list,\n",
    "                                 padding_list, pool_kernel_list, pool_stride_list):\n",
    "        \"\"\"\n",
    "        Calculates the shape of the final output assuming that a every conv layer is followed\n",
    "        by a pooling layer.\n",
    "        \"\"\"\n",
    "         #### your code here ###\n",
    "\n",
    "         #### end code here ###\n",
    "        return(int(finalOutputShape))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.convolutions(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.finalLayer(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Model Summary\n",
    "Using the summary function from torchsummary print out the layers, shapes, and number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Dynamic CNN\n",
    "Now we can train our model. First train the original model implemented above. It shoud train just fine and achieve decent accuracy. \n",
    "\n",
    "1. Your first task is to implement early stopping by keeping track of your best accuracy and stopping training if accuracy doesn't improve after 3 checks. I suggest that you check your evaluation accuracy every 50 iterations unless you have a GPU then by all means check whenever you want. Hell, check twice an iteration. Ain't nothing stopping you with a GPU. Once your best model is found, quit out and stop running. This is where you'd usually save a model, but don't worry about doing that.\n",
    "\n",
    "2. Your second task is to experiment with different configurations. Try at least two architectural changes (depth, convolution layers, channels, strides, **optimization**, average pooling, etc) and record some observations about model performance for your two configurations. I would suggest using Adam as an optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myIter = 0\n",
    "bestAcc = 0\n",
    "patience = 2# how many times should we be ok with our accuracy not increasing?\n",
    "checksWithoutIncrease = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if checksWithoutIncrease == patience:\n",
    "            break\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        myIter += 1\n",
    "        if myIter % 50 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images\n",
    "                images = images.requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            # Check if early stopping criteria are met\n",
    "            #### your code here ###\n",
    "\n",
    "            #### end code here ###\n",
    "            print('\\tIteration: {}. Loss: {}. Testing Accuracy: {}'.format(myIter, loss.item(), accuracy))\n",
    "    # remember, early stopping is qutting out of all training.\n",
    "    #### your code here ###\n",
    "\n",
    "    #### end code here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
