{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww17080\viewh8480\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Training Procedure\

\f1\b0 1. Define a neural network that has parameters that we will learn (
\f2\i aka
\f1\i0  weights)\
2. Iterate over a dataset of inputs (
\f2\i aka 
\f1\i0 1 epoch)\
	2a. Process every input through the network\
	2b. Compute the loss (using the preselected 
\f2\i objective function
\f1\i0 )\
	2c. Propagate gradients back into the network\'92s parameters (
\f2\i aka
\f1\i0  backpropigation) \
		- this is accomplished by tracking all computations performed on the learnable parameters\
\

\f0\b Important Concepts\

\f1\b0 The fundamental procedure for learning model parameters in deep learning is through 
\f0\b backpropigation
\f1\b0 . First, we build a computational graph for each vector that tracks the operations that were performed on that vector. Second, we then use the computational graph to compute the 
\f2\i partial derivative of the loss function with respect to each model weight. 
\f1\i0 These partial derivatives give us the 
\f0\b gradient
\f1\b0  of the loss function. We then update the model weights using 
\f0\b gradient descent
\f1\b0 . In short, the gradient descent algorithm attempts to find a 
\f2\i local minimum
\f1\i0  of the loss function by iteratively updating the weights of the model using the gradient computed using backpropigation. Gradient descent uses a simple update rule that subtracts the gradient (multiplied by the learning rate) from the model parameters. }