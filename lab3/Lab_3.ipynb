{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spicy_meme](https://jeinson.github.io/images/mem2.jpg)\n",
    "\n",
    "<center>Depends who you ask...</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Regression with Stochatic Gradient Descent \n",
    "\n",
    "In the previous lab, we introduced auto differentiation, then used gradient descent to maximize the likelihood for simple model parameters. Today, we'll build on this knowledge by implementing linear regression using pytorch's Neural Network package, `torch.nn`. \n",
    "\n",
    "To review, in least squares linear regression we have a design matrix $X$ and a set of corresponding outcomes $Y$, and the goal is to learn a $\\beta$ such that $\\hat{Y} = X^\\top \\beta + \\epsilon$ minimizes the loss function $\\frac{1}{n}\\sum_n (Y - \\hat{Y})^2$, with $\\mathbb{E}[\\epsilon] = 0$\n",
    "\n",
    "In the simple case of one feature and one output (see above meme):\n",
    "\n",
    "$$ \n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n \n",
    "\\end{bmatrix}  , \n",
    "X = \\begin{bmatrix}\n",
    "1 & X_1 \\\\\n",
    "1 & X_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & X_n \n",
    "\\end{bmatrix} ,\n",
    "\\beta = \n",
    " \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \n",
    "\\end{bmatrix}, \\text{and } \n",
    "\\epsilon = \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\ \n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In $X$, the leading column of 1s allows for an intercept parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common method used for solving linear equations is the **Method of Least Squares**. This is what popular packages like scikit learn or R's `lm` function are doing under the hood. With some fun matrix algebra, the solution pops out fairly easily. \n",
    "\n",
    "$$\\begin{align}\n",
    "X\\beta &= Y\\\\\n",
    "X^\\top X\\beta &= X^\\top Y \\\\\n",
    "(X^\\top X)^{-1}X^\\top X \\beta&= (X^\\top X)^{-1} X^\\top Y \\\\\n",
    "&\\implies \\\\\n",
    "\\hat{\\beta}&= (X^\\top X)^{-1} X^\\top Y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This solution involves taking the inverse of a matrix, which gets computationally very expensive as the size of your dataset increases. Therefore, we can think of the problem of optimizing $\\beta$ in terms of the machine learning workflow discussed in class, where essentially, we try to mimize the quantity $Q$ in \n",
    "$$ (X^\\top \\beta -  Y)^\\top (X^\\top \\beta - Y) = Q $$\n",
    "The matrix calculus needed to optimize $\\beta$ is pretty ugly, but fortunately we can use autodifferentiation and gradient descent to arrive at an optimal solution. The task of modeling linear regression can be thought of in terms of the machine learning workflow discussed in class. \n",
    "\n",
    "![Learning Algorithm](https://jeinson.github.io/images/Learning_Algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following modules. These are the only ones needed to complete the lab. \n",
    "# -.5 if you import any other modules ;-)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch tensors, randomly draw a `[2 x 1] `$\\beta$ vector, and 50 random $X_i$s for training. Then matrix multiply $X$ and $\\beta$, and add some Gaussian noise, to make a $Y$ vector. This will be the training set used for training your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLSmodel(X):\n",
    "    beta = torch.rand(1,1) # randomly initialize model parameters \n",
    "    return X*beta.t()\n",
    "\n",
    "# create random X\n",
    "X = torch.rand(50,1)# 2D matrix with intercept & 1 randomly initialized parameter\n",
    "# create output (Y) \n",
    "Y = torch.from_numpy(np.add(OLSmodel(X).data.numpy(), # OLSmodel output in an numpy array\n",
    "                    np.random.normal(scale = 0.04,size=np.shape(product)))) # add Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot X and Y to verify their linear relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11a288f98>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solve Ordinary Least Squares\n",
    "### Loss Function: \n",
    "#### (ùëã‚ä§ùõΩ‚àíùëå)‚ä§(ùëã‚ä§ùõΩ‚àíùëå) = ùëÑ\n",
    "Using the loss function presented above, find the least squares solution for your sample dataset. This will be accomplished by using Pytorchs' built-in optimization algorithms. \n",
    "\n",
    "To use torch.optim,  you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients. The parameters will be updated over multiple epochs (as conducted by calling `optimizer.step()`. See more below:\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7233c912d1cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOLS_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# define the loss function\n",
    "def OLS_loss(X, beta, Y):\n",
    "    return torch.mm(((X.mm(beta).t().double()) - Y), ((X.mm(beta).t().double()) - Y))\n",
    "\n",
    "# optimizer object will hold the current state and will update the parameters based on the computed gradients.\n",
    "optimizer = torch.optim.SGD([beta], lr = 0.01, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam([var1, var2], lr = 0.0001)\n",
    "\n",
    "# update parameters\n",
    "optimizer.zero_grad()\n",
    "loss = OLS_loss(X, beta, Y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Jack's solution\n",
    "Y_hat = OLSmodel(X)\n",
    "Mse = (Est_y - real_y).pow(2).sum()\n",
    "(Something).zero_grad()\n",
    "Mse.backward()\n",
    "Optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot a trend line on your data using your least squares estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Defining the model as a neural network\n",
    "\n",
    "In pytorch, neural networks are are typically constructed by defining a class that includes at the very least a `forward` method. Some information on the `torch.nn` module is [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "Fill in the missing pieces to make a neural network class called `LinearRegression` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    # The class constructor defines the parameters (ie layers) of the neural network\n",
    "    def __init__(self):\n",
    "        # salling Super Class's constructor\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # This neural network will have a single linear layer\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    # The forward() method ties the layers together to build the network\n",
    "    # We take the gradient of this composite function using back propogation\n",
    "    def forward(self, x):\n",
    "        # the forward pass is simply a linear function\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "input_dim, output_dim = 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, the model initializes with random parameters. Define a new LinearRegression object, and print the values of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is defining a loss function, which our model will attempt to miminize during training, and an optimization function, which specifies the learning method and learning rate. The most commonly used optimizer is **Stochastic Gradient Descent**, which is conveniently implemented as `torch.optim.SGD`. More on this [here](https://pytorch.org/docs/stable/optim.html). Which loss function should you use to optimizer the parameters in linear regression? [Check here for some options](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Define a loss object called `criterion` and an optimizer object called `optimizer`, for the parameters of your new `LinearRegression` object, and with a learning rate of .01. (You may have to play around with the learning rate if your model diverges off to infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Mean Squared Loss\n",
    "learningRate = 0.01\n",
    "optimiser = torch.optim.SGD(linear_model.parameters(), lr = learningRate) # Stochastic Gradient Descent\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train the model. This is where the magic happens! In training, we iterate through the process of choosing parameters, calculating the loss, then marching down the gradient until the parameters converge. Fill in the following code chunk to optimize the parameters of `lr_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Estimate Y_hat with the current model\n",
    "\n",
    "    # Compute the loss\n",
    "        \n",
    "    # Compute the gradient of the loss, and update the model parameters with the optimizer\n",
    "    # Don't forget to zero the gradient after each epoch!\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"loss {}, beta 1 {}\".format(loss,list(lr_model.parameters())[1].detach().numpy()))\n",
    "        \n",
    "        \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch +=1\n",
    "    #increase the number of epochs by 1 every time\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_correct))\n",
    "\n",
    "    #clear grads as discussed in prev post\n",
    "    optimiser.zero_grad()\n",
    "    #forward to get predicted values\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()# back props\n",
    "    optimiser.step()# update the parameters\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the dataset with a trend line defined by the parameters your neural network learned, in addition to the OLS trend line, and label which is which. Try setting the alpha if they end up right on top of each other, or moving one of the lines over a bit so they are both easy to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool huh? I'll leave it up to you to decide which method you like better, but hopefully this demonstrates the flexibility of a neural network. Later on, we'll see how this package can be used to learn far more complicated patterns in data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)\n\n",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
